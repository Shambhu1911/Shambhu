s16
Q1
<html>
<head>
<script>
$(document).ready(function() {
    $('#bookSelect').change(function() {
        var selectedBook = $(this).val();
        $.ajax({
            url: 'getBookDetails.php',
            method: 'POST',
            data: { selectedBook: selectedBook },
            dataType: 'json',
            success: function(response) {
                $('#bookDetails').html(
                    '<p>Title: ' + response.title + '</p>' +
                    '<p>Author: ' + response.author + '</p>' +
                    '<p>Year: ' + response.year + '</p>' +
                    '<p>Price: $' + response.price + '</p>'
                );
            }
        });
    });
});
</script>
</head>
<body>
<label for="bookSelect">Select a Book:</label>
<select id="bookSelect">
    <option value="">Select</option>
    <option value="The Great Gatsby">The Great Gatsby</option>
    <option value="To Kill a Mockingbird">To Kill a Mockingbird</option>
    <option value="1984">1984</option>
</select>
<div id="bookDetails"></div>
</body>
</html>

<?php
if(isset($_POST['selectedBook'])) {
    $selectedBook = $_POST['selectedBook'];
    
    $xml = simplexml_load_file('books.xml');
    
    foreach ($xml->book as $book) {
        if ($book->title == $selectedBook) {
            $response = array(
                'title' => (string)$book->title,
                'author' => (string)$book->author,
                'year' => (int)$book->year,
                'price' => (float)$book->price
            );
            echo json_encode($response);
            break;
        }
    }
}
?>

Q2
import nltk
 nltk.download('stopwords')
 nltk.download('punkt')
 import re
 from nltk.corpus import stopwords
 from nltk.tokenize  import word_tokenize,sent_tokenize

 te=" Text semmerisation is an nlp technique that extract text from a large amount of data. It is the process of identifying the most meanigful information in a document and compressing it into a shorter version by preserving its meaning. @ $ Types 12 : Extractive Text Summerisation "
 te=re.sub(r'\[[0-9]*\]', ' ', te)
 te=re.sub(r'\s+', ' ', te)
 te=re.sub(r'\[[0-9]{}*\]' ,' ', te)
 ft=re.sub('[^a-zA-Z]', ' ' ,te)
 print("\n Text after removing Special char , symbols and digits \n")
 print(ft)
 print("\n")
 sw=set(stopwords.words("english"))
 print("\n Stopwords..... \n")
 print(sw)
 wd=word_tokenize(ft)
 print("\n Word Tokens..... \n")
 print(wd)
 print("\n Extractive Text Summery \n ")
 wordfreq = {}
 for word in wd:
   if word in sw:
       continue
   if word in wordfreq:
      wordfreq[word] += 1
   else:
     wordfreq[word] = 1
 maximum_frequency = max(wordfreq.values())
 for word in wordfreq.keys():
     wordfreq[word] = (wordfreq[word]/maximum_frequency)
 sentences = sent_tokenize(ft)
 sentenceValue = {}
 for sentence in sentences:
    for word, freq in wordfreq.items():
        if word in sentence.lower():
           if sentence in sentenceValue:
                sentenceValue[sentence] += freq
           else:
               sentenceValue[sentence] = freq
 import heapq
 summary = ''
 summary_sentences = heapq.nlargest(4, sentenceValue, key=sentenceValue.get)
 summary = ' '.join(summary_sentences)
 print(summary)



import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize, sent_tokenize
import re
nltk.download('stopwords')
nltk.download('punkt')

te = "Text semmerisation is an nlp technique that extract text from a large amount of data. It is the process of identifying the most meanigful information in a document and compressing it into a shorter version by preserving its meaning. @ $ Types 12 : Extractive Text Summerisation"

te = re.sub(r'\[[0-9]*\]', ' ', te)
te = re.sub(r'\s+', ' ', te)
te = re.sub(r'\[[0-9]{}*\]' ,' ', te)

ft = re.sub('[^a-zA-Z]', ' ' ,te)
print("\nText after removing Special char , symbols and digits \n")
print(ft)
print("\n")

sw = set(stopwords.words("english"))
print("\nStopwords..... \n")
print(sw)

wd = word_tokenize(ft)
print("\nWord Tokens..... \n")
print(wd)

print("\nExtractive Text Summary\n")

wordfreq = {}
for word in wd:
    if word in sw:
        continue
    if word in wordfreq:
        wordfreq[word] += 1
    else:
        wordfreq[word] = 1

maximum_frequency = max(wordfreq.values())

for word in wordfreq.keys():
    wordfreq[word] = (wordfreq[word] / maximum_frequency)

sentences = sent_tokenize(ft)

sentenceValue = {}
for sentence in sentences:
    for word, freq in wordfreq.items():
        if word in sentence.lower():
            if sentence in sentenceValue:
                sentenceValue[sentence] += freq
            else:
                sentenceValue[sentence] = freq

import heapq
summary_sentences = heapq.nlargest(4, sentenceValue, key=sentenceValue.get)
summary = ' '.join(summary_sentences)
print(summary)